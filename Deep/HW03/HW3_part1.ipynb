{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_part1.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part I: Using CNN**"
      ],
      "metadata": {
        "id": "qW1zYUNXNgcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "emqtf99lf0Jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tygsUeQ5fjlA"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Sequential, Input, Model \n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data preparation**"
      ],
      "metadata": {
        "id": "TcgWeIsIOemB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Loading data set using libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "5g9b-CI7FSHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X,train_Y), (test_X,test_Y) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "hga88Pxrfytr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Print size of train and test sets**"
      ],
      "metadata": {
        "id": "54sQ4mzWGD4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shape: ', train_X.shape, train_Y.shape)\n",
        "print('Testing data shape: ', test_X.shape, test_Y.shape)"
      ],
      "metadata": {
        "id": "56UDxqg07wHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Count number of classes of data set**"
      ],
      "metadata": {
        "id": "QRlKqDSFGlgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(train_Y)\n",
        "nclasses = len(classes)\n",
        "print('Total number of outputs: ', nclasses)\n",
        "print('Outputnclasses: ', classes)"
      ],
      "metadata": {
        "id": "ZUrXXM2V9Psp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Show two figures of dataset** (one of them from train set and another one from test set)"
      ],
      "metadata": {
        "id": "tCp5lspoGrmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "plt.imshow(train_X[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "plt.imshow(test_X[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(test_Y[0]))"
      ],
      "metadata": {
        "id": "pg8t7DT89abk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reshape train and test set**"
      ],
      "metadata": {
        "id": "N63lSn0BHW8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_X.reshape(-1, 28, 28, 1)\n",
        "test_X = test_X.reshape(-1, 28, 28, 1)\n",
        "train_X.shape, test_X.shape"
      ],
      "metadata": {
        "id": "3eVBuzd1F3Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert types of train and test sets**"
      ],
      "metadata": {
        "id": "ifiRnRxVHbXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_X.astype('float32')\n",
        "test_X = test_X.astype('float32')\n",
        "train_X = train_X / 255\n",
        "test_X = test_X / 255"
      ],
      "metadata": {
        "id": "w5_ruQKlF6_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert class vectors to binary class matrix** for using categorical cross entropy as loss function in our CNN"
      ],
      "metadata": {
        "id": "mbV1lfC0Hi9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y_one_hot = to_categorical(train_Y)\n",
        "test_Y_one_hot = to_categorical(test_Y)\n",
        "print('Original label:', train_Y[0])\n",
        "print('After conversion to one-hot', train_Y_one_hot[0])"
      ],
      "metadata": {
        "id": "cLF9ph56-650"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test set processing**"
      ],
      "metadata": {
        "id": "OfMxy0ZCOmBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split test set into two parts**: 1. test 2.valid\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cHo5OC5BIHYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x, valid_x, test_label, valid_label = train_test_split(test_X, test_Y_one_hot, test_size = 0.5, random_state = 1)"
      ],
      "metadata": {
        "id": "pKUNYz7DGsH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Show sizes of valid and test sets**"
      ],
      "metadata": {
        "id": "ntRHlHP5IUut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.shape, valid_x.shape, test_label.shape, valid_label.shape"
      ],
      "metadata": {
        "id": "uXPYmSWheDTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **First part for showing events underfit and overfit**"
      ],
      "metadata": {
        "id": "xnbWUamqOsBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialize size of batch, number of epochs, and number of classes** for using in our CNN"
      ],
      "metadata": {
        "id": "5Ug8EumOIcWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 20\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "itwI2giJGKza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create our CNN  model**"
      ],
      "metadata": {
        "id": "yJg6S7EkIpP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model = Sequential()\n",
        "fashion_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape=(28,28,1), padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))\n",
        "fashion_model.add(MaxPooling2D((2,2), padding = 'same'))\n",
        "\n",
        "fashion_model.add(Conv2D(64, (3, 3), activation = 'linear', padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))\n",
        "fashion_model.add(MaxPooling2D(pool_size = (3,3), padding = 'same'))\n",
        "\n",
        "fashion_model.add(Conv2D(128, (3, 3), activation = 'linear', padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))                  \n",
        "fashion_model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "\n",
        "fashion_model.add(Flatten())\n",
        "fashion_model.add(Dense(128, activation = 'linear'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))                  \n",
        "fashion_model.add(Dense(num_classes, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "WEO3ZFJBG9pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compile our CNN model**"
      ],
      "metadata": {
        "id": "GnjimHweI2NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model.compile(loss = keras.losses.categorical_crossentropy, optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "q_XBTBNzJZPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of our CNN model**"
      ],
      "metadata": {
        "id": "N07LsA3xI7VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model.summary()"
      ],
      "metadata": {
        "id": "OW7NSpZNJbpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Teach our CNN model using train and valid sets**"
      ],
      "metadata": {
        "id": "0rEGdqxSJBWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_train = fashion_model.fit(train_X, train_Y_one_hot, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))"
      ],
      "metadata": {
        "id": "ltp_LJesJdkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluate our CNN model on test set**(5000 samples)"
      ],
      "metadata": {
        "id": "pI34qx81JLKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_eval = fashion_model.evaluate(test_x, test_label, verbose = 0)"
      ],
      "metadata": {
        "id": "M7vfHLWsJg59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test loss: ', test_eval[0])\n",
        "print('Test accuracy: ', test_eval[1])"
      ],
      "metadata": {
        "id": "hsDO8aCfJmMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analyze the performance of our CNN model on train and valid sets**\n",
        "\n",
        "اHere, We have both the event of overfit and underfit because\n",
        "1. In first chart, our CNN model works better on train set than valid set. \n",
        "2. In second chart, our CNN model works better on valid set than train set."
      ],
      "metadata": {
        "id": "37zS54gAJ2SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = fashion_train.history['accuracy']\n",
        "val_accuracy = fashion_train.history['val_accuracy']\n",
        "loss = fashion_train.history['loss']\n",
        "val_loss = fashion_train.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label = 'Validation accuracy')\n",
        "plt.title('Trainimg and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rahK6yOTJp_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Second part with underfit and overfit events**"
      ],
      "metadata": {
        "id": "kdGEWTVmPHCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialize size of batch, number of epochs, and number of classes** for using in our CNN"
      ],
      "metadata": {
        "id": "uqKpSLAFKoae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 20\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "m636HdDzJsZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create our CNN  model**"
      ],
      "metadata": {
        "id": "Nr1UCG45K1eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model = Sequential()\n",
        "fashion_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape=(28,28,1), padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))\n",
        "fashion_model.add(MaxPooling2D((2,2), padding = 'same'))\n",
        "fashion_model.add(Dropout(0.25))\n",
        "\n",
        "fashion_model.add(Conv2D(64, (3, 3), activation = 'linear', padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))\n",
        "fashion_model.add(MaxPooling2D(pool_size = (2,2), padding = 'same'))\n",
        "fashion_model.add(Dropout(0.25))\n",
        "\n",
        "fashion_model.add(Conv2D(128, (3, 3), activation = 'linear', padding = 'same'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1))                  \n",
        "fashion_model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "fashion_model.add(Dropout(0.4))\n",
        "\n",
        "fashion_model.add(Flatten())\n",
        "fashion_model.add(Dense(128, activation = 'linear'))\n",
        "fashion_model.add(LeakyReLU(alpha = 0.1)) \n",
        "fashion_model.add(Dropout(0.3))\n",
        "fashion_model.add(Dense(num_classes, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "N8QQoy5PJ87-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of our CNN model**"
      ],
      "metadata": {
        "id": "0KKp6ZUbK9dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model.summary()"
      ],
      "metadata": {
        "id": "KjKGffCPKMDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compile our CNN model**"
      ],
      "metadata": {
        "id": "EScjwNnMLEct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model.compile(loss = keras.losses.categorical_crossentropy, optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "f5tsKeC5KP1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Teach our CNN model using train and valid sets**"
      ],
      "metadata": {
        "id": "BmR3BSieLMjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_train_dropout = fashion_model.fit(train_X, train_Y_one_hot, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (valid_x, valid_label))"
      ],
      "metadata": {
        "id": "AzmCL-_xKQo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save our model** for using on test set(5000 samples)"
      ],
      "metadata": {
        "id": "VpURtuoMLQES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_model.save(\"fashion_model_dropout.h5py\")"
      ],
      "metadata": {
        "id": "ERFobp41KTED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test our CNN model on test set(5000 samples)**"
      ],
      "metadata": {
        "id": "DKICOjCnLZZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_eval = fashion_model.evaluate(test_x, test_label, verbose = 1)"
      ],
      "metadata": {
        "id": "tD2QDLtuKVqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test loss: ', test_eval[0])\n",
        "print('Test accuracy: ', test_eval[1])"
      ],
      "metadata": {
        "id": "giBwyDCJKYVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analyze the performance of our CNN model on train and valid sets**\n",
        "\n",
        "اHere, We don't have neither the event of overfit nor underfit because our CNN model works on train set as well as valid set. "
      ],
      "metadata": {
        "id": "_XXuP2uGL0bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = fashion_train_dropout.history['accuracy']\n",
        "val_accuracy = fashion_train_dropout.history['val_accuracy']\n",
        "loss = fashion_train_dropout.history['loss']\n",
        "val_loss = fashion_train_dropout.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fM0bsmALKa2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test our second CNN model on test set(5000 samples)**"
      ],
      "metadata": {
        "id": "y3tCyW17L_7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictes_classes = fashion_model.predict(test_x)"
      ],
      "metadata": {
        "id": "_rUi9c4hKfz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = np.argmax(np.round(predictes_classes), axis = 1)"
      ],
      "metadata": {
        "id": "YpZZANt9Kgsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label_correct = np.argmax(np.round(test_label), axis = 1)"
      ],
      "metadata": {
        "id": "tSRVOcyiKi35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Show numbers of correct classification**\n",
        "We show some images as sample."
      ],
      "metadata": {
        "id": "DluCsCyfMKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = np.where(predicted_classes == test_label_correct)[0]\n",
        "print (\"Found %d correct labels\" % len(correct))\n",
        "for i, correct in enumerate(correct[:9]):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(test_X[correct].reshape(28, 28), cmap = 'gray', interpolation = 'none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_label_correct[correct]))    \n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "CY5sEUk5Kk2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Show numbers of incorrect classification**\n",
        "We show some images as sample."
      ],
      "metadata": {
        "id": "O_fbZpRVMRlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect = np.where(predicted_classes!=test_label_correct)[0]\n",
        "print (\"Found %d incorrect labels\" % len(incorrect))\n",
        "for i, incorrect in enumerate(incorrect[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(test_X[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_Y[incorrect]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "GjmnM7wuKp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final evaluation of our second CNN model**"
      ],
      "metadata": {
        "id": "cn3y0Q8bMepA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "print(classification_report(test_label_correct, predicted_classes, target_names=target_names))"
      ],
      "metadata": {
        "id": "yLpfSoJHD-Qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}